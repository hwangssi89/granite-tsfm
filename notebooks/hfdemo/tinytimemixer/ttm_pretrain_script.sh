python ttm_pretrain_sample.py  --context_length 512 \
                               --forecast_length 96 \
                               --patch_length 8 \
                               --d_model 24 \
                               --decoder_d_model 24 \
                               --batch_size 64 \
                               --num_layers 5 \
                               --decoder_num_layers 3 \
                               --dropout 0.7 \
                               --head_dropout 0.7 \
                               --early_stopping 1 \
                               --adaptive_patching_levels 0 \
                               --num_epochs 5 \
                               --multi_scale 0 \
                               --register_tokens 0 \
                               --fft_length 0 \
                               --patch_gating 0 \
                               --scaling revin \
                               --multi_scale_loss 0 \
                               --use_fft_embedding 1 \
                               --enable_fourier_attention 0 \
                               --disable_pad_activations 1 \